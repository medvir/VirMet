{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to VirMet","text":"<p>VirMet is a software suite designed to help users running viral metagenomics (mNGS) experiments: unspecific massively parallel NGS sequencing with the aim of discovering and characterizing the virus fraction of biological samples.</p> <p>To see and download the code, visit our GitHub.</p> <p>VirMet is called with a command-subcommand syntax. All the possible subcommands are:</p> <ul> <li><code>fetch</code>: download genomes</li> <li><code>update</code>: update viral/bacterial database</li> <li><code>index</code>: index genomes</li> <li><code>wolfpack</code>: analyze a Miseq run</li> <li><code>covplot</code>: plot coverage for a specific organism</li> </ul> <p>Some help can be obtained with <code>virmet &lt;subcommand&gt; -h</code>.</p> <p>As an example, if you want to download the viral nucleotide database, you can use it as follows: <code>virmet fetch --viral n</code>.</p> <p>Enjoy using VirMet!</p> <p></p>"},{"location":"Covplot/","title":"Inspecting the results: <code>covplot</code>","text":""},{"location":"Covplot/#covplot","title":"Covplot","text":"<p>After a run of <code>wolfpack</code> we have a count of how many reads in a sample are assigned to a given organism, but we might be interested to know what fraction of the genome is covered by our reads because this would give further evidence to the presence of the organism in our sample. In other words, thirty reads from three different regions of the genome provide a stronger evidence than thirty reads all from the same region. With <code>covplot</code> we can easily create a plot of the coverage for an organism of interest.</p> <p>Let's suppose we want to investigate sample <code>AR-1_S1</code> in the directory <code>virmet_output_exp_01</code>; we first look at the file listing organisms and reads count</p> <pre><code>[user@host test_virmet]$ cat virmet_output_exp_01/AR-1_S1/orgs_list.tsv\norganism    reads\nHuman adenovirus 7  126\nHuman poliovirus 1 strain Sabin 45\nHuman poliovirus 1 Mahoney  29\nHuman adenovirus 3+11p  19\nHuman adenovirus 16 1\n</code></pre> <p>This seems to be populated by two viruses, some adenovirus and some polivirus. Let's run <code>covplot</code> with <code>--help</code> to list the available options</p> <pre><code>[user@host test_virmet]$ virmet covplot --help\nusage: virmet &lt;command&gt; [options] covplot [-h] [--outdir OUTDIR]\n                                          [--organism ORGANISM]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --outdir OUTDIR      path to sample results directory\n  --organism ORGANISM  name of the organism as reported in orgs_list.tsv file\n</code></pre> <p>If we want the coverage of reads on the genome of adenovirus we can run</p> <pre><code>[user@host test_virmet]$ virmet covplot --outdir virmet_output_exp_01/AR-1_S1 \\\n--organism \"Human adenovirus\"\n</code></pre> <p><code>covplot</code> will perform the following steps:</p> <ol> <li>identify all mappings read-organism where the organism name starts with \"Human adenovirus\",</li> <li>identify the organism with the highest number of reads mapped,</li> <li>download the genome from Genbank and align all viral reads to it,</li> <li>compute the coverage, write it to <code>depth.txt</code> and plot it.</li> </ol> <p>The final result is a pdf file <code>Human_adenovirus_coverage.pdf</code>. Regarding point 1, it is important to note that giving <code>\"Human adenovirus\"</code> will chose the genome with most hits from adenovirus 7 (top in the list). If one wants to see how well viral reads would cover another adenovirus then one needs to give, e.g., <code>--organism \"Human adenovirus 3\"</code>.</p> <p></p>"},{"location":"Installation/","title":"Installation Guide","text":"<p>VirMet is available through Bioconda, a channel for the conda package manager. Once conda is installed and the channels are set up, <code>conda install virmet</code> installs the package with all its dependencies.</p>"},{"location":"Installation/#dependencies","title":"Dependencies","text":"<p>The classic <code>python setup.py install</code> should work, provided the user has the necessary permission. VirMet relies on a number of third-party tools used to access databases, trim, convert, filter and map reads: these are automatically installed if VirMet is installed with <code>conda install</code>. If, for some reason, the user prefers to manually install everything, the tools VirMet depends on are:</p> <ul> <li>biopython (Python package)</li> <li>blast &gt;= 2.3</li> <li>bwa</li> <li>entrez-direct (E-utilities from the command line)</li> <li>htslib</li> <li>pandas (Python package)</li> <li>prinseq</li> <li>python (3.x)</li> <li>R with ggplot2 library</li> <li>samtools &gt;= 1.3</li> <li>seqtk</li> </ul> <p>Alternatively, we provide a docker image with everything installed, see instructions for dockerised VirMet.</p>"},{"location":"Installation/#commands","title":"Commands","text":"<p>On a Ubuntu 14.04 the following commands should provide a system wide installation of the tools mentioned above (sudo required).</p> <pre><code># system wide configuration available as Ubuntu packages\nsudo apt-get update -qq\nsudo apt-get install -qq -y build-essential ftp golang unzip \\\nbwa tabix seqtk libwww-perl r-cran-ggplot2\n\n#  NCBI edirect tools\ncd /tmp\nperl -MNet::FTP -e \\\n  '$ftp = new Net::FTP(\"ftp.ncbi.nlm.nih.gov\", Passive =&gt; 1); $ftp-&gt;login;\n   $ftp-&gt;binary; $ftp-&gt;get(\"/entrez/entrezdirect/edirect.zip\");'\nunzip -u -q edirect.zip\nrm edirect.zip\nexport PATH=$PATH:/tmp/edirect\n./edirect/setup.sh\ncd edirect\nsudo install -p econtact edirutil efilter elink entrez-phrase-search eproxy \\\nespell ftp-cp join-into-groups-of sort-uniq-count-rank xtract xtract.Linux \\\neaddress edirect.pl efetch einfo enotify epost esearch esummary ftp-ls nquire \\\nreorder-columns setup-deps.pl sort-uniq-count word-at-a-time xtract.pl /usr/local/bin\n\n# prinseq\ncd /tmp\nwget http://downloads.sourceforge.net/project/prinseq/standalone/prinseq-lite-0.20.4.tar.gz \\\n-O /tmp/prinseq-lite-0.20.4.tar.gz\ntar -xvf /tmp/prinseq-lite-0.20.4.tar.gz\nsudo install -p tmp/prinseq-lite-0.20.4/prinseq-lite.pl /usr/local/bin\n\n# samtools 1.3\nwget https://github.com/samtools/samtools/releases/download/1.3/samtools-1.3.tar.bz2 \\\n-O /tmp/samtools-1.3.tar.bz2\ntar xvfj /tmp/samtools-1.3.tar.bz2\ncd /tmp/samtools-1.3\nmake\nsudo make prefix=/usr/local install\n\n# NCBI blast+ 2.3.0\ncd /tmp\nwget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/ncbi-blast-2.3.0+-x64-linux.tar.gz\ntar xzfp ncbi-blast-2.3.0+-x64-linux.tar.gz\nsudo install -p ./ncbi-blast-2.3.0+/bin/* /usr/local/bin\n\n# edirect looks for uname in the wrong place\nsudo ln -sv /bin/uname /usr/bin/uname\n\nexport PATH=/usr/local/bin:$PATH\n</code></pre> <p>Then, one needs python 3 (VirMet was mainly developed and tested on 3.4/3.5), but any 3.x should work), together with pandas and Biopython. Go to the respective installation pages and choose your favourite method.</p>"},{"location":"Preparation/","title":"Preparing for a real run: <code>fetch</code> or <code>upload</code>, and <code>index</code>","text":""},{"location":"Preparation/#fetch","title":"Fetch","text":"<p>After installation, one needs to populate the database directory. By default this will be <code>/data/virmet_databases</code> and will occupy about 60 GB. In order to populate this, use the subcommand <code>fetch</code>, for example as follows</p> <pre><code>virmet fetch --viral n  # this downloads viral sequences, nucleotide only\nvirmet fetch --human\nvirmet fetch --bact\nvirmet fetch --fungal\nvirmet fetch --bovine\n</code></pre> <p>This might take long. If it's taking too long, you might want to include the above commands in a <code>down.sh</code> file and run them overnight.</p> <p>Only the option <code>--viral</code> takes an argument: <code>n</code> for nucleotide and <code>p</code> for protein viral database. Currently only nucleotide sequences are used, while the protein ones are foreseen as useful in discovery of novel viral sequences (in a future version).</p>"},{"location":"Preparation/#update","title":"Update","text":"<p>More and more sequences are uploaded to NCBI database every month.</p> <p>VirMet provides a simple way to update the viral database without the need to download all the genomes again. This can be done with the subcommand <code>update</code> as in the example</p> <pre><code>virmet update --viral n\n</code></pre> <p>Similarly, new bacterial sequences can be added as well as fungal. Human and bovine database, since they consist of a single organism, don't need to be updated so often.</p> <p>Don't forget to index the database again once it has been updated.</p>"},{"location":"Preparation/#adding-sequences-manually","title":"Adding sequences manually","text":"<p>By adding the switch <code>--picked file_with_ids</code> users can add sequences by writing their ids in a file, one per line.</p>"},{"location":"Preparation/#index","title":"Index","text":"<p>After dowloading or updating the databases, it's always needed to index them. The subcommand <code>virmet index</code> is used for that, and it can take multiple arguments,  so you can run</p> <pre><code>virmet index --viral n --human --bact --fungal --bovine\n</code></pre> <p>and wait for the indexing to finish.</p>"},{"location":"Wolfpack/","title":"Running a virus scan: <code>wolfpack</code>","text":""},{"location":"Wolfpack/#wolfpack","title":"Wolfpack","text":"<p>This can be run on a single file or on a directory. It will try to guess from the naming scheme if it is a Miseq output directory (i.e. with <code>Data/Intensities/BaseCalls/</code> structure) and analyze all fastq files in there. The extension must be <code>.fastq</code> or <code>.fastq.gz</code>. It will then run a filtering step based on quality, length and entropy (in short: reads with a lot of repeats will be discarded), followed by a decontamination step where reads of human/bacterial/bovine/fungal origin will be discarded. Finally, remaining reads are blasted against the viral database. The list of organisms with the count of reads is in files <code>orgs_list.csv</code> in the output directory (naming is <code>virmet_output_...</code>). For example, if we have a directory named <code>exp_01</code> with files</p> <pre><code>exp_01/AR-1_S1_L001_R1_001.fastq.gz\nexp_01/AR-2_S2_L001_R1_001.fastq.gz\nexp_01/AR-3_S3_L001_R1_001.fastq.gz\nexp_01/AR-4_S4_L001_R1_001.fastq.gz\n</code></pre> <p>we could run</p> <pre><code>virmet wolfpack --run exp_01\n</code></pre> <p>and, after some time, find the results in <code>virmet_output_exp01</code>. Many files are present, the most important ones being <code>orgs_list.csv</code> and <code>stats.tsv</code>. The first lists the viral organisms found with a count of reads that could be matched to them.</p> <pre><code>[user@host test_virmet]$ cat virmet_output_exp_01/AR-1_S1/orgs_list.tsv\norganism    reads\nHuman adenovirus 7  126\nHuman poliovirus 1 strain Sabin 45\nHuman poliovirus 1 Mahoney  29\nHuman adenovirus 3+11p  19\nHuman adenovirus 16 1\n</code></pre> <p>The second file is a summary of all reads analyzed for this sample and how many were passing a specific step of the pipeline or matching a specific database.</p> <pre><code>[user@host test_virmet]$ cat virmet_output_exp01/AR-1_S1/stats.tsv\nraw_reads       6250\ntrimmed_too_short       462\nlow_entropy     1905\nlow_quality     0\npassing_filter  3883\nmatching_humanGRCh38    3463\nmatching_bact1  0\nmatching_bact2  0\nmatching_bact3  0\nmatching_fungi1 0\nmatching_bt_ref 0\nreads_to_blast  420\nviral_reads     257\nundetermined_reads      163\n</code></pre>"},{"location":"Wolfpack/#additional-files","title":"Additional files","text":"<p>At the end of a run a directory for each sample (fastq file analyzed) is created containing the following files:</p> <pre><code>good_humanGRCh38_bact1_bact2_bact3_fungi1_bt_ref.cram\ngood_humanGRCh38_bact1_bact2_bact3_fungi1_bt_ref.err\n...\ngood_humanGRCh38_bact1.cram\ngood_humanGRCh38_bact1.err\ngood_humanGRCh38.cram\ngood_humanGRCh38.err\n\norgs_list.tsv\nprinseq.err\nprinseq.log\nstats.tsv\nundetermined_reads.fastq.gz\nunique.tsv.gz\nviral_reads.fastq.gz\n</code></pre> <p>Files <code>orgs_list.tsv</code> and <code>stats.tsv</code> report the main output of the tool as reported above, while <code>unique.tsv.gz</code> reports blast hits to viral database.</p> <p>As the names say, <code>viral_reads.fastq.gz</code> and <code>undetermined_reads.fastq.gz</code> contain, respectively, reads identified as of viral origin and reads not matching any of the considered genomes.</p> <p><code>prinseq.err</code> and <code>prinseq.log</code> are, respectively, the standard error and log file of prinseq, used to filter reads. By inspecting this log file, VirMet determines how many reads were discarded because of low entropy or low quality.</p> <p>In the decontamination step, reads are aligned against human genome first, those matching are discarded while those not matching are aligned against the first set of bacterial genomes, and so on. File <code>good_humanGRCh38.cram</code> is the alignment of high quality reads (good) to human genome, saved in CRAM format. File <code>good_humanGRCh38_bact1.cram</code> contains the alignment to bacterial genomes in set bact1 of high quality reads (good) minus those that were identified as matching human genome, and so on. File ending in <code>err</code> contain the standard error of the conversion bam -&gt; cram.</p>"},{"location":"Wolfpack/#hot-run","title":"Hot run","text":"<p>A virus scan on a full MiSeq run typically lasts a few hours, many of which are spent in the decontamination phase. Sometimes, after a run is completed, we would like to run it again with a new viral database. In these cases, <code>wolfpack</code> would run skipping the previous phases to save time. It relies on the presence of intermediate files that, if present, signals the pipeline that a specific step must be skipped.</p> <p>These are the rules (must be intended for each sample):</p> <ul> <li>if both <code>prinseq.log</code> and <code>prinseq.err</code> exist, skip quality filtering,</li> <li>if <code>good_humanGRCh38.err</code> exists, skip the human reads decontamination,</li> <li>if <code>good_humanGRCh38_bact1.err</code> exists, skip the decontamination against first bacterial   database, and so on.</li> </ul> <p>Blasting against viral database will always be performed. If both <code>viral_reads.fastq.gz</code> and <code>undetermined_reads.fastq.gz</code> exist, their content will be copied into a file, they will be removed, and this new file will be blasted against the viral database.</p> <p>In short, if we change the viral database after a run has already been analyzed, simply running <code>virmet wolfpack</code> again will skip the quality filtering and go straight to blast against viral database.</p>"}]}